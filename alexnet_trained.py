# -*- coding: utf-8 -*-
"""Alexnet trained.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VClTfE4QUT6l_OIxJWLGkaWcnCPfHqoV
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import cv2
import pickle
import tensorflow as tf

N_IMAGES=400
data_dir = '/content/drive/My Drive/data/train'
categories=['Tomato__Target_Spot','Tomato__Tomato_mosaic_virus','Tomato_Bacterial_spot','Tomato_Early_blight','Tomato_healthy','Tomato_Late_blight','Tomato_Leaf_Mold']

data=[]
def make_data():
    for category in categories:
        path=os.path.join(data_dir,category)
        label=categories.index(category)
        
        for img_name in os.listdir(path)[:N_IMAGES]:
            image_path=os.path.join(path,img_name)
            image=cv2.imread(image_path)
            
            try:
                image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                image=cv2.resize(image,(224,224))
                
                image=np.array(image,dtype=np.float32)
                data.append([image,label])
            except Exception as e:
                pass
    pik=open('data.pickle','wb')
    pickle.dump(data,pik)
    pik.close()
    print(len(data))

make_data()

def load_data():
    pick=open('data.pickle','rb')
    data=pickle.load(pick)
    pick.close()
    
    np.random.shuffle(data)
    
    feature=[]
    labels=[]
    
    for img,label in data:
        feature.append(img)
        labels.append(label)
    
    feature=np.array(feature,dtype=np.float32)
    labels=np.array(labels)
    feature=feature/255.0
    
    return[feature,labels]

load_data()

from sklearn.model_selection import train_test_split

(feature,labels)=load_data()
x_train,x_test,y_train,y_test=train_test_split(feature,labels,test_size=0.2)

#layers
input_layer=tf.keras.layers.Input([224,224,3])
conv1=tf.keras.layers.Conv2D(filters=96,kernel_size=(11,11),strides=4,padding='Same',activation='relu')(input_layer)
pool1=tf.keras.layers.MaxPooling2D(pool_size=(3,3),strides=(2,2))(conv1)
conv2=tf.keras.layers.Conv2D(filters=256,kernel_size=(5,5),strides=1,padding='Same',activation='relu')(pool1)
pool2=tf.keras.layers.MaxPooling2D(pool_size=(3,3),strides=(2,2))(conv2)

conv3=tf.keras.layers.Conv2D(filters=384,kernel_size=(3,3),strides=1,padding='Same',activation='relu')(pool2)
conv4=tf.keras.layers.Conv2D(filters=384,kernel_size=(3,3),strides=1,padding='Same',activation='relu')(conv3)
conv5=tf.keras.layers.Conv2D(filters=256,kernel_size=(3,3),strides=1,padding='Same',activation='relu')(conv4)
pool3=tf.keras.layers.MaxPooling2D(pool_size=(3,3),strides=(2,2))(conv5)


flt1=tf.keras.layers.Flatten()(pool3)

dn1=tf.keras.layers.Dense(4096,activation='relu')(flt1)
dn2=tf.keras.layers.Dense(4096,activation='relu')(dn1)
out=tf.keras.layers.Dense(7,activation='softmax')(dn2)

#train model
model=tf.keras.Model(input_layer,out)
model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])

model.summary()

history=model.fit(x_train,y_train,batch_size=100,epochs=60)

import matplotlib.pyplot as plt
def plot(history):
    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,4))
    axes[0].plot(history.history['loss'])   
    #axes[0].plot(history.history['val_loss'])
    axes[0].legend('loss')#['loss','val_loss']

    axes[1].plot(history.history['accuracy'])   
    #axes[1].plot(history.history['val_accuracy'])
    axes[1].legend(['accuracy'])

plot(history)

model.evaluate(x_test,y_test,verbose=1)
prediction=model.predict(x_test)
plt.figure(figsize=(9,9))
for i in range(9):
    plt.subplot(3,3,i+1)
    plt.imshow(x_test[i])
    plt.xlabel('Actual:'+categories[y_test[i]]+'\n'+'predicted:'+categories[np.argmax(prediction[i])])
    plt.xticks([])

model.save('mymodel.h5')

pip install livelossplot

from livelossplot import PlotLossesKeras

model.fit(x_train, y_train,
          epochs=10,
          validation_data=(x_test, y_test),
          callbacks=[PlotLossesKeras()],
          verbose=0)